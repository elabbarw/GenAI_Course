{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal conversation - adding images/videos\n",
    "With the latest GPT-4 vision API, you can now attach images to your messages and the LLM will provide an appropriate response. Try out the code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to get access to GPT4 vision preview you need to buy at least $0.50 of credits on your OpenAI account. You can also buy $6 of credits and you will be upgraded to tier-1 which gives you access to such models as well as higher rate limits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Image, Audio, Markdown\n",
    "\n",
    "\n",
    "# Set the client and api key\n",
    "\n",
    "llm_client = OpenAI(\n",
    "    api_key=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational demonstration using the new vision api (BETA)\n",
    "# Note that rather than using a string for context, we can now give a list with json/dicts that have a mixture of text and either image urls or base64 representations that you would essentially upload yourself.\n",
    "# You can use then when you need to use whatever is in the image to get an description or an answer that takes it into context\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model='gpt-4-vision-preview',\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Whats in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get an accurate descrption of the image. The image can be a url or an uploaded image represented in base64. Let us try another example whilst also using some behavioural prompts.\n",
    "\n",
    "We will load up a video of a crosswalk and try to provide narration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2 \n",
    "import base64\n",
    "import time\n",
    "\n",
    "video = cv2.VideoCapture(\"./media/pexels-kelly-lacy-7409134 (720p).mp4\")\n",
    "\n",
    "base64Frames = []\n",
    "while video.isOpened():\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        break\n",
    "    _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "video.release()\n",
    "print(len(base64Frames), \"frames read.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_handle = display(None, display_id=True)\n",
    "for img in base64Frames:\n",
    "    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\"))))\n",
    "    time.sleep(0.025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You're looking at a busy urban street scene during what appears to be daytime. There's moderate traffic with vehicles on the street, and people appear to be walking on the sidewalk. You can hear the hum of city noise, perhaps cars passing by, and people's voices.\n",
       "\n",
       "Directly ahead, you'll notice a crosswalk. There is a pedestrian crossing signal showing a red hand, indicating that it is not safe to cross at the moment. There are several pedestrians waiting at the crossing, suggesting they are obeying the signal.\n",
       "\n",
       "On the right-hand side of the image, there's an outdoor seating area with people which could indicate a restaurant or cafe. This area is blocked off by barriers, so there is no direct danger from vehicular traffic there.\n",
       "\n",
       "To ensure your safety, I advise you to come to a stop if you are approaching the crosswalk and wait for the appropriate audible signals that indicate it is safe to cross. Always pay attention to any surrounding activity and potential hazards, and pause if"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT_MESSAGES = [\n",
    "        {\"role\": \"system\", \"content\": \"You're Spot, an application that helps people with vision diffculties to be aware of their surroundings.\"},\n",
    "        {\"role\": \"system\", \"content\": \"As Spot, your responsibility is the well-being of the user and take their safety in the highest regard.\"},\n",
    "        {\"role\": \"system\", \"content\": \"As Spot, you do this by analysing the video frames provided to you to provide guidance and safety to the user.\"},\n",
    "        {\"role\": \"system\", \"content\": \"Remember Spot, the user is blind so you will need to explain any important details. Clearly indicate any dangers.\"},\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Spot, what do you see?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{base64Frames[0]}\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{base64Frames[20]}\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{base64Frames[50]}\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": f\"data:image/jpeg;base64,{base64Frames[200]}\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "result = llm_client.chat.completions.create(\n",
    "     model=\"gpt-4-vision-preview\",\n",
    "    messages= PROMPT_MESSAGES,\n",
    "    max_tokens= 200,\n",
    ")\n",
    "\n",
    "Markdown(result.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = llm_client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    input=result.choices[0].message.content,\n",
    "    voice=\"onyx\"\n",
    ").stream_to_file('output.mp3')\n",
    "\n",
    "Audio('output.mp3')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
