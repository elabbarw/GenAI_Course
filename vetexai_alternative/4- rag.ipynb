{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empowering Chatbots with Retrieval Augmented Generation (RAG)!\n",
    "\n",
    "Imagine supercharging your chatbot with the ability to swiftly comb through the vast resources of the internet or specific databases, enabling it to provide more up-to-date and insightful answers. This is where the magic of RAG comes into play!\n",
    "\n",
    "Here's the scoop: When a user poses a question to your chatbot, instead of solely relying on its internal knowledge, the chatbot sends the query to an API, such as an Internet Search or VectorDB. It then leverages the results to construct a more comprehensive and informed response. Think of it as your chatbot conducting quick online research before delivering an answer!\n",
    "\n",
    "## What Makes RAG So Remarkable?\n",
    "\n",
    "- Cost-Effective: Unlike the resource-intensive process of fine-tuning, RAG is more lightweight and easier to manage.\n",
    "- Human Analogy: Consider your information needs. Would you attempt to absorb the entirety of a library's content, or would you locate the precise book and extract what you require? RAG operates on the latter principle. \n",
    "\n",
    "For businesses, instead of laboriously sifting through heaps of documents to train your chatbot, you can simply store them in a VectorDB. When a query arises, your chatbot can retrieve the pertinent information and present it in a user-friendly format.\n",
    "\n",
    "## RAG vs. Fine-Tuning\n",
    "\n",
    "- Choose Fine-Tuning when you need to refine your chatbot's behavior.\n",
    "- Opt for RAG when you want your chatbot to tap into external knowledge bases. Bonus: RAG works seamlessly even with smaller 7B models!\n",
    "\n",
    "Our journey begins with a beginner's example, addresses challenges, and delves into advanced use cases in the world of RAG. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-aiplatform\n",
    "%pip install requests\n",
    "%pip install duckduckgo_search\n",
    "%pip install gptrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import ChatModel, InputOutputTextPair\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "vertexai.init() ## initalise the vertexai class\n",
    "\n",
    "chat_model = ChatModel.from_pretrained(\"chat-bison\")\n",
    "\n",
    "parameters = {\n",
    "    \"temperature\": 0.7,  # Temperature controls the degree of randomness in token selection.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, when utilizing zero-shot learning, the responses generated are rooted in the LLM's training data, which is limited to information available up to its training cutoff date. Example has been ammened due to Vertex cutoff date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = \"\\n\".join([  ### Vertex doesnt use the same concept of sending a list like openai - instead, i am taking the list and joining it togather as one string seperated by a newline (\\n)\n",
    "\n",
    "])\n",
    "\n",
    "chat = chat_model.start_chat(  ### We start our chat with the context\n",
    "    context=memory\n",
    ")\n",
    "\n",
    "response = chat.send_message( ## Send a new message along with parameters\n",
    "    message=\"What happened to Matthew Perry?\", **parameters)\n",
    "\n",
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's introduce a function that enables us to leverage DuckDuckGo and adapt the prompt in a way that allows us to present both the query and the search results concurrently..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG - DuckDuckGo\n",
    "from duckduckgo_search import DDGS\n",
    "from gptrim import trim\n",
    "\n",
    "def search_internet(query):\n",
    "    \"\"\"\n",
    "    Use Duckduck go Search\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 5\n",
    "    result_text=''\n",
    "    with DDGS() as ddgs:\n",
    "        search_results = [r for r in ddgs.text(query, max_results=count)]\n",
    "        for result in search_results:\n",
    "            title = result.get('title', '')\n",
    "            snippet = result.get('body', '')\n",
    "            url = result.get('href', '')\n",
    "            result_text += f'Title: {title}\\nSnippet: {snippet}\\nURL: {url}\\n\\n'\n",
    "\n",
    "        search_prompt = f\"\"\"\n",
    "        Based on the internet search results provided in <>, provide an answer to the query [] if it is relevant along with a source URL. \\\n",
    "        If there are no internet search results or if they are not relevant then say \\\"Please try again.\\\"\\\n",
    "        \n",
    "        context:<{trim(result_text)}>\n",
    "        query:[{query}]\n",
    "        \"\"\"\n",
    "\n",
    "        return search_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our function at our disposal, let's approach the task by first sending our query to the DuckDuckGo function and then supplying the adapted prompt to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEP 1 - ASK THE QUESTION\n",
    "question = 'What happened to Matthew Perry?'\n",
    "\n",
    "\n",
    "# STEP 2 - SUBMIT THE QUESTION TO AN API TO GET RESULTS AND CREATE A NEW PROMPT FOR THE LLM\n",
    "\n",
    "prompt = search_internet(question)\n",
    "\n",
    "\n",
    "# STEP 4 - SEND THE MODIFIED PROMPT TO THE LLM \n",
    "\n",
    "memory =[]\n",
    "\n",
    "memory.append(prompt)\n",
    "\n",
    "chat = chat_model.start_chat(  ### We start our chat with the context\n",
    "    context=\"\\n\".join(memory)\n",
    ")\n",
    "\n",
    "response = chat.send_message( ## Send a new message along with parameters\n",
    "    message=question, **parameters)\n",
    "\n",
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's delve into the challenges at hand:\n",
    "\n",
    "### Challenge 1 - Simplifying Complex Queries\n",
    "One prominent issue is the substantial text required to extract a concise answer. This not only impacts comprehension but also results in the usage of more tokens, potentially incurring additional costs. However, you can employ modules like \"gptrim\" to condense the text by approximately 50%, eliminating spaces, stop words, and the like. Remarkably, the LLM, particularly GPT-3 and 4, remains adept at understanding and responding effectively to such condensed input. However, you might get mixed results from VertexAI's chat-bison and other models.\n",
    "\n",
    "### Challenge 2 - Transforming User Queries into Search Queries\n",
    "Accepting the user's query as-is doesn't always lead to optimal results. To address this, we need a mechanism to modify the query into an effective search query. Here, the LLM can assist in generating appropriate search queries using the function calling API. While several frameworks like Langchain and Semantic Kernel are available, I have found that function calling offers a straightforward and practical approach.\n",
    "\n",
    "### Challenge 3 - Efficient Decision-Making\n",
    "Consider that this setup operates within a chatbot application, across platforms such as websites, Microsoft Teams, Slack, Alexa, or robotic interfaces. In such scenarios, we don't want to trigger internet searches unnecessarily. To optimize this, we can introduce functions with distinct names and descriptions to the LLM's function calling API. The LLM then possesses the discretion to determine whether a function call is warranted, allowing for context-appropriate responses.\n",
    "\n",
    "### Challenge 4 - Accessing Company Documents\n",
    "Intriguingly, there's a desire to engage with company documents effectively. While you can explore this avenue with Confluence, Stack Overflow, and Sharepoint APIs, it's important to note that these solutions rely on keyword-based searches. This necessitates the exact matching of keywords found in document titles for retrieval. Alternatively, leveraging a vector database introduces the capacity for semantic searches. Here, you can import all your documents or text snippets and harness the power of semantic search by querying the concept or idea most closely aligned with the content you seek. We'll delve into a practical example of this in a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Function Calling\n",
    "\n",
    "In this approach, we entrust the LLM with the authority to discern when a function (often referred to as \"skills\") is necessary, generate the appropriate search query, and then incorporate the results into the LLM to obtain the final answer.\n",
    "\n",
    "Here's the breakdown of this process:\n",
    "\n",
    "1. **Function Development:** We create functions that act as additional \"eyes\" (read) and \"hands\" (actions/write) for the LLM.\n",
    "\n",
    "2. **Function Definitions:** We define these functions in a list, specifying the exact function name and a description. The LLM utilizes this information to select the relevant function. We also define the properties required for each function.\n",
    "\n",
    "3. **Query Submission:** Your query is submitted to the LLM, along with the function definitions.\n",
    "\n",
    "4. **Function Requirement:** If the LLM determines that a function is required, you will receive a response that includes the necessary function and its associated properties.\n",
    "\n",
    "5. **Function Execution:** You utilize the information provided by the LLM to internally trigger and execute the required function. This could involve searching the internet, performing specific actions, or any other defined tasks.\n",
    "\n",
    "6. **Final Answer:** After executing the function, you return to the LLM with the required information, and the LLM generates the final response.\n",
    "\n",
    "This approach streamlines the interaction between the LLM and the external functions, enhancing the model's ability to deliver precise and context-aware responses.\n",
    "\n",
    "\n",
    "## VertexAI's chat-bison doesnt have such functionality, as this behaviour is fine-tuned with the latest version of OpenAI models. Therefore, i will compensate for this by providing my own behavioural prompts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to run the search_internet function before this!\n",
    "import json\n",
    "\n",
    "## Requires a function \n",
    "QUESTION = \"Who is Twitter's CEO in 2023?\"\n",
    "## Doesn't require a function\n",
    "#QUESTION = \"How can i make a cup of tea?\"\n",
    "\n",
    "\n",
    "# Let us define the search_internet function for the LLM...\n",
    "skill_definitions = [\n",
    "      {\n",
    "        \"name\": \"search_internet\",\n",
    "        \"description\": \"Searches the internet to provide context to a query if required\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The query to search for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "tool_memory =['You only answer in JSON format with nothing else.'] ## Set the behaviour so we can extract JSON.\n",
    "message_memory = []\n",
    "\n",
    "tool_memory.append(prompt)\n",
    "\n",
    "chat = chat_model.start_chat(  ### We start our chat with the context\n",
    "    context=\"\\n\".join(tool_memory)\n",
    "    ### Examples can be used to demonstrate how tools would be selected if required...\n",
    ")\n",
    "\n",
    "\n",
    "special_prompt = f\"\"\"\n",
    "You have access to a JSON toolbox of functions with a name, description and required arguments as shown within []. Please look at the query submitted in <> to establish if \\n\n",
    "a tool is required to answer the question, or else act as a helpful assistant.\\n\n",
    "\n",
    "Please provide a JSON response using the following keys:\n",
    "tool_required(true/false),func_name,func_arguments, message_no_tool\n",
    "\n",
    "toolbox: {skill_definitions}\n",
    "query: {QUESTION}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "first_response = chat.send_message( ## Send a new message along with parameters\n",
    "    message=special_prompt, **parameters)\n",
    "\n",
    "\n",
    "response_json = json.loads(first_response.text) ### Get the response and load as python dict.\n",
    "\n",
    "print (response_json)\n",
    "\n",
    "### Do we need a skill? True\n",
    "if response_json['tool_required']:\n",
    "\n",
    "    # tie response with our functions\n",
    "    available_functions = {\n",
    "        'search_internet': search_internet\n",
    "    }\n",
    "\n",
    "\n",
    "    # extract function name and arguments from the LLM\n",
    "    function_name = response_json['func_name']\n",
    "    function_args = response_json['func_arguments']\n",
    "\n",
    "    function2call = available_functions.get(function_name)\n",
    "\n",
    "    \n",
    "\n",
    "    # call it appropriately\n",
    "    function_response = function2call(**function_args)\n",
    "\n",
    "\n",
    "    # send the message history to the LLM again without functions.\n",
    "    chat = chat_model.start_chat(  ### We start our chat with the context\n",
    "        context=\"\\n\".join(message_memory)\n",
    "        \n",
    "    )\n",
    "\n",
    "    final_response = chat.send_message( ## Send a new message along with parameters\n",
    "    message=function_response, **parameters)\n",
    "    \n",
    "    # Result\n",
    "\n",
    "    print (str(final_response.text))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "else: ## No skill required - False then output the message normally\n",
    "    print(response_json['message_no_tool']) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, the LLM has determined the necessity of a tool and has furnished the essential arguments for its execution. Notably, the query was also appropriately modified to align with this requirement.\n",
    "\n",
    "While this approach is effective, it's crucial to be vigilant, as the LLM may occasionally propose function names that do not exist, or not act according to your instructions at all! Implementing a conditional checks to ensure that the returned function name aligns with the predefined ones is advisable to maintain control and accuracy.\n",
    "\n",
    "With this approach, you have the flexibility to integrate as many \"hands\" and \"eyes\" as necessary into your chatbot, enabling it to adapt and extend its capabilities as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector & RAG using VertexAI\n",
    "Unfortunately, due to VertexAI API limitations, i cannot provide examples involving a VectorDB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
