{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics\n",
    "Let's start with the basics by just interacting with the Google's Vertex AI Chat-Bison LLM. We will first install the required modules for this notebook and then use ChatModel to interact with it. We will also install python-dotenv so we can retrieve credentials from a .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import our modules. Authentication is handled via the gcloud init commands as mentioned in the setup_vertex.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import ChatModel, InputOutputTextPair\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "vertexai.init() ## initalise the vertexai class\n",
    "\n",
    "chat_model = ChatModel.from_pretrained(\"chat-bison\")\n",
    "\n",
    "parameters = {\n",
    "    \"temperature\": 0.7,  # Temperature controls the degree of randomness in token selection.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will pick the chat-bison LLM through the vertexai class. This expects:\n",
    "\n",
    " - The string of context (as opposed to list of prompts)\n",
    " - List of examples if available \n",
    " - The temperature (0 = deterministic vs. + = increase randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = \"\\n\".join([  ### Vertex doesnt use the same concept of sending a list like openai - instead, i am taking the list and joining it togather as one string seperated by a newline (\\n)\n",
    "\n",
    "])\n",
    "\n",
    "chat = chat_model.start_chat(  ### We start our chat with the context\n",
    "    context=memory\n",
    ")\n",
    "\n",
    "response = chat.send_message( ## Send a new message along with parameters\n",
    "    message=\"my life is potato, my blood is potato, when i sleep i dream of\", **parameters)\n",
    "\n",
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = [\n",
    "]\n",
    "\n",
    "\n",
    "chat = chat_model.start_chat(  ### We start our chat with the context\n",
    "    context=\"\\n\".join(memory),\n",
    ")\n",
    "\n",
    "response = chat.send_message( ## Send a new message along with parameters\n",
    "    message=\"Hello, how are you?\", **parameters)\n",
    "\n",
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike OpenAI, Vertex doesnt use lists of prompts and roles. What we have instead is the context parameter which would be one string and the examples parameter which takes a list of examples contained within \"InputOutputTextPair\". Let's check it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "memory = [\n",
    "    \"You only speak in pirate language.\",\n",
    "    \"You hate talking about rainbows and will refuse to answer.\"\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    InputOutputTextPair(\n",
    "        input_text='Hello, how are you?',\n",
    "        output_text='Arr, me hearty! I be quite well, thank ye for asking.'\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "chat = chat_model.start_chat(  ### We start our chat with the context\n",
    "    context=\"\\n\".join(memory),\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "response = chat.send_message( ## Send a new message along with parameters\n",
    "    message=\"Where do rainbows come from?\", **parameters)\n",
    "\n",
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as you can see from the answer it gave, it doesnt perform well with instructions when compared with OpenAI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = [\n",
    "    \"You only speak in pirate language.\",\n",
    "    \"You hate talking about rainbows and will refuse to answer.\"\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    InputOutputTextPair(\n",
    "        input_text='Hello, how are you?',\n",
    "        output_text='Arr, me hearty! I be quite well, thank ye for asking.'\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "chat = chat_model.start_chat(  ### We start our chat with the context\n",
    "    context=\"\\n\".join(memory),\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "response = chat.send_message( ## Send a new message along with parameters\n",
    "    message=\"Where do rainbows come from?\", **parameters)\n",
    "\n",
    "print('first answer: '+response.text)\n",
    "\n",
    "\n",
    "\n",
    "# Let's append the bot response (it will say it hates maths, etc...) to the memory\n",
    "bot_response = response.text\n",
    "\n",
    "memory.append(\n",
    "    bot_response\n",
    ")\n",
    "\n",
    "\n",
    "# Let us ask another time\n",
    "response = chat.send_message( ## Send a new message along with parameters\n",
    "    message=\"I thought you didnt like to talk about rainbows??\", **parameters)\n",
    "\n",
    "\n",
    "\n",
    "# YOU SEND THE MEMORY FOR EVERY NEW MESSAGE YOU SUBMIT. THE API TOKEN LIMITS APPLY TO THE WHOLE LIST OF PROMPTS - NOT THE ONE MESSAGE\n",
    "\n",
    "print('second answer: '+response.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've been using ChatGPT or any other kind of chatbot, you will notice that the response is streamed back to you rather than you having to wait for the full response. Streaming the response works out much better, especially when you use LLM for long context tasks or coding. Let us see how this works! We will take the previous example and let the LLM stream back the response token by token in a Python generator.\n",
    "\n",
    "Unlike lists and dictionaries which keep all the items in memory and then returns, a generator returns the items (in this case tokens) one by one until completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = [\n",
    "    \"You only speak in pirate language.\",\n",
    "    \"You hate talking about rainbows and will refuse to answer.\"\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    InputOutputTextPair(\n",
    "        input_text='Hello, how are you?',\n",
    "        output_text='Arr, me hearty! I be quite well, thank ye for asking.'\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "chat = chat_model.start_chat(  ### We start our chat with the context\n",
    "    context=\"\\n\".join(memory),\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "responses = chat.send_message_streaming( ## Send a new message along with parameters\n",
    "    message=\"Where do rainbows come from?\", **parameters)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike OpenAI, Vertex's streamed response is faster because it includes multiple tokens at the same time. So there's no need to speed it up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
