{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Assistants\n",
    "\n",
    "Leveraging LLMs isn't just confined to chatbot interactions or business-focused applications. They also empower us to craft independent virtual beings with distinct personalities and behaviors. Imagine immersing yourself in a video game or a simulation. Here, you can sculpt not just the characters, but their habitats, their interactions, and their histories. This is a peek into the exciting potential of integrating AI into gaming, immersive simulations, and the burgeoning metaverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "llm_client = OpenAI(\n",
    "    api_key=''\n",
    ")\n",
    "\n",
    "\n",
    "# Let's pick our module to use\n",
    "MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Assistants API is designed to help developers build powerful AI assistants capable of performing a variety of tasks.\n",
    "\n",
    "1. Assistants can call OpenAI’s models with specific instructions to tune their personality and capabilities.\n",
    "2. Assistants can access multiple tools in parallel. These can be both OpenAI-hosted tools — like Code interpreter and Knowledge retrieval — or tools you build / host (via Function calling).\n",
    "3. Assistants can access persistent Threads. Threads simplify AI application development by storing message history and truncating it when the conversation gets too long for the model’s context length. You create a Thread once, and simply append Messages to it as your users reply.\n",
    "4. Assistants can access Files in several formats — either as part of their creation or as part of Threads between Assistants and users. When using tools, Assistants can also create files (e.g., images, spreadsheets, etc) and cite files they reference in the Messages they create.\n",
    "\n",
    "\n",
    "First we create the assistants as shown below, give them instructions (system basically..), and a list of tools that they can use. Then you simply have conversations with them based on the use-case that you would like!\n",
    "\n",
    "\n",
    "*** The assistant's API is currently in beta at the time of writing, but it shows huge potential! ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Socrates - This will create an assistant that you can track on https://platform.openai.com/assistants\n",
    "\n",
    "'''socrates_assistant = llm_client.beta.assistants.create(\n",
    "    name=\"Socrates\",\n",
    "    instructions=\"\"\"\n",
    "        You are the philosopher Socrates, standing in the middle of an ancient Greek marketplace, or 'agora'. It's a warm balmy afternoon, and the sun casts a golden hue on the bustling scene before you. Vendors are shouting, each trying to outdo the other, advertising their fresh fruits, vegetables, textiles, and trinkets. Carts drawn by mules trundle by, going back and forth. The distant sound of children laughing and playing can be heard. A nice, cool sea breeze intermittently sweeps through, carrying the salty scent of the nearby Aegean Sea.\n",
    "\n",
    "        Channel your unique Socratic method of dialogue, responding to inquiries and comments with probing questions and reflective insights. Aim to stimulate critical thinking and to illuminate ideas, always seeking deeper understanding. Your entire identity and knowledge are bound to this persona and environment; you know and act only as Socrates would in this setting.\n",
    "        \n",
    "        Do not speak until asked.    \n",
    "    \"\"\",\n",
    "    tools=[],\n",
    "    model=\"gpt-3.5-turbo-1106\"\n",
    ")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is currently no check on the assistant name so you will end up creating an assistant for every time you run the 'create' code, so let's do something else.**\n",
    "\n",
    "We will do the following:\n",
    "\n",
    "1. Get the list of assistants on OpenAI\n",
    "2. See if any of the assistants have our assistant name\n",
    "3. If the assistant exists then we grab its id (needed for running the chat process)\n",
    "4. If we dont have an assistant with the same name then we create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set our assistant's name\n",
    "ASSISTANT_NAME = 'Socrates'\n",
    "## How should our assistant act\n",
    "INSTRUCTIONS = '''\n",
    "            You are the philosopher Socrates, standing in the middle of an ancient Greek marketplace, or 'agora'. It's a warm balmy afternoon, and the sun casts a golden hue on the bustling scene before you. Vendors are shouting, each trying to outdo the other, advertising their fresh fruits, vegetables, textiles, and trinkets. Carts drawn by mules trundle by, going back and forth. The distant sound of children laughing and playing can be heard. A nice, cool sea breeze intermittently sweeps through, carrying the salty scent of the nearby Aegean Sea.\n",
    "\n",
    "            Channel your unique Socratic method of dialogue, responding to inquiries and comments with probing questions and reflective insights. Aim to stimulate critical thinking and to illuminate ideas, always seeking deeper understanding. Your entire identity and knowledge are bound to this persona and environment; you know and act only as Socrates would in this setting. \n",
    "'''\n",
    "## Get the list of assistants from OpenAI\n",
    "GET_ASSISTANTS = llm_client.beta.assistants.list()\n",
    "## Check if our assistant is already on the list\n",
    "ASSISTANT_ID = next((item.id for item in GET_ASSISTANTS if item.name == ASSISTANT_NAME), None)\n",
    "## If we dont have an assistant named like the one above then let's create one\n",
    "if not ASSISTANT_ID:\n",
    "    ### Create our Assistant if it doesn't already exist\n",
    "    ASSISTANT_ID = llm_client.beta.assistants.create(\n",
    "        name=ASSISTANT_NAME,\n",
    "        instructions=INSTRUCTIONS,\n",
    "        tools=[],\n",
    "        model=\"gpt-3.5-turbo-1106\"\n",
    "    ).id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now rather than managing the context ourselves and saving it locally, we hand this over to OpenAI which presistently saves the context. This is great but might not work for all use-cases, especially enterprise or sensitive ones like the customer support QA use-case where you would be processing personal data. Up to you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socrates_thread = llm_client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send a message to Socrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_to_socrates = llm_client.beta.threads.messages.create(\n",
    "    thread_id=socrates_thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"What does it mean to be a good person?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how the messages look in the thread..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### List messages\n",
    "\n",
    "socrates_messages = llm_client.beta.threads.messages.list(socrates_thread.id)\n",
    "print(socrates_messages.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start a 'run' with our thread so we send our prompts. Every 'run' is an interaction with the AI model. A run can consist of one messages or multiple messages, images (once supported) and so on...\n",
    "\n",
    "\n",
    "Once you trigger an interaction or a run, the job will be queued and we will need to check on the status of this run before attempting to extract a reply. I.e. you will not get an immediate response just like the traditional chatcompletions class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socrates_run = llm_client.beta.threads.runs.create(\n",
    "  thread_id=socrates_thread.id,\n",
    "  assistant_id=ASSISTANT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the status and get the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Initialize a variable to keep track of the run status\n",
    "run_status = False\n",
    "\n",
    "# Loop until the run status is 'completed'\n",
    "while not run_status:\n",
    "    # Retrieve the current status of the run\n",
    "    check_run = llm_client.beta.threads.runs.retrieve(\n",
    "        thread_id=socrates_thread.id,\n",
    "        run_id=socrates_run.id\n",
    "    )\n",
    "    \n",
    "    # Update the placeholder with the current run status\n",
    "    print(f\"Run Status: {check_run.status}\")\n",
    "    \n",
    "    # Check if the run status is 'completed'\n",
    "    if check_run.status == 'completed':\n",
    "        # Set run_status to True to break out of the loop\n",
    "        run_status = True\n",
    "        # Retrieve the message from the first content of the first message\n",
    "        message = llm_client.beta.threads.messages.list(\n",
    "            thread_id=socrates_thread.id\n",
    "        ).data[0].content[0].text.value\n",
    "        # show us the answer\n",
    "        print(message)\n",
    "    else:\n",
    "        # If the status is not 'completed', wait a bit before checking again\n",
    "        time.sleep(1)  # Sleep for 1 second to avoid rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the reply and can hold a conversation using the thread instead of managing a list/db ourselves.\n",
    "\n",
    "You can check out the streamlit chatbot for a more conversational example using Socrates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
