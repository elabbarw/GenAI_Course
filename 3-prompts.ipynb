{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "In the realm of artificial intelligence, GenAI prompt engineering is a pivotal field. It encompasses various techniques, including zero-shot and few-shot learning, chain of thought, directional stimulus prompting, and reactive prompting, to enhance the capabilities of LLMs like GPT. These techniques enable AI models to perform a wide range of tasks, maintain logical conversations, and respond contextually to user input. This notebook explores these techniques, their applications, and their potential to unlock the true power of AI in different domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "from openai import OpenAI\n",
    "import IPython\n",
    "\n",
    "\n",
    "# Set the client and api key\n",
    "\n",
    "llm_client = OpenAI(\n",
    "    api_key=''\n",
    ")\n",
    "# Let's set our model\n",
    "MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crafting Precise Instructions and Directives\n",
    "When formulating prompts for straightforward tasks, you can effectively guide the model by employing clear commands. These instructions, such as \"Write,\" \"Classify,\", \"Extract\", \"Summarize,\" \"Translate,\" \"Order,\" \"Complete,\" and more, specify the desired action and help the model understand your intended outcome. \n",
    "\n",
    "Let us look at the following examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Complete the sentence: the sky color is\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Classify the following sentence: I really hated the show!\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Translate into Arabic: I love potato, my life is potato. When i sleep, i dream of potato.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Summarise the following text: Tell me, O muse, of that ingenious hero who travelled far and wide after he had sacked the famous town of Troy. Many cities did he visit, and many were the nations with whose manners and customs he was acquainted; moreover he suffered much by sea while trying to save his own life and bring his men safely home; but do what he might he could not save his men, for they perished through their own sheer folly in eating the cattle of the Sun-god Hyperion; so the god prevented them from ever reaching home. Tell me, too, about all these things, O daughter of Jove, from whatsoever source you may know them. \n",
    "\n",
    "So now all who escaped death in battle or by shipwreck had got safely home except Ulysses, and he, though he was longing to return to his wife and country, was detained by the goddess Calypso, who had got him into a large cave and wanted to marry him. But as years went by, there came a time when the gods settled that he should go back to Ithaca; even then, however, when he was among his own people, his troubles were not yet over; nevertheless all the gods had now begun to pity him except Neptune, who still persecuted him without ceasing and would not let him get home. \n",
    "\n",
    "Now Neptune had gone off to the Ethiopians, who are at the world's end, and lie in two halves, the one looking West and the other East. He had gone there to accept a hecatomb of sheep and oxen, and was enjoying himself at his festival; but the other gods met in the house of Olympian Jove, and the sire of gods and men spoke first. At that moment he was thinking of Aegisthus, who had been killed by Agamemnon's son Orestes; so he said to the other gods: \n",
    "\n",
    "\"See now, how men lay blame upon us gods for what is after all nothing but their own folly. Look at Aegisthus; he must needs make love to Agamemnon's wife unrighteously and then kill Agamemnon, though he knew it would be the death of him; for I sent Mercury to warn him not to do either of these things, inasmuch as Orestes would be sure to take his revenge when he grew up and wanted to return home. Mercury told him this in all good will but he would not listen, and now he has paid for everything in full.\" \n",
    "\n",
    "Then Minerva said, \"Father, son of Saturn, King of kings, it served Aegisthus right, and so it would any one else who does as he did; but Aegisthus is neither here nor there; it is for Ulysses that my heart bleeds, when I think of his sufferings in that lonely sea-girt island, far away, poor man, from all his friends. It is an island covered with forest, in the very middle of the sea, and a goddess lives there, daughter of the magician Atlas, who looks after the bottom of the ocean, and carries the great columns that keep heaven and earth asunder. This daughter of Atlas has got hold of poor unhappy Ulysses, and keeps trying by every kind of blandishment to make him forget his home, so that he is tired of life, and thinks of nothing but how he may once more see the smoke of his own chimneys. You, sir, take no heed of this, and yet when Ulysses was before Troy did he not propitiate you with many a burnt sacrifice? Why then should you keep on being so angry with him?\" \n",
    "\n",
    "And Jove said, \"My child, what are you talking about? How can I forget Ulysses than whom there is no more capable man on earth, nor more liberal in his offerings to the immortal gods that live in heaven? Bear in mind, however, that Neptune is still furious with Ulysses for having blinded an eye of Polyphemus king of the Cyclopes. Polyphemus is son to Neptune by the nymph Thoosa, daughter to the sea-king Phorcys; therefore though he will not kill Ulysses outright, he torments him by preventing him from getting home. Still, let us lay our heads together and see how we can help him to return; Neptune will then be pacified, for if we are all of a mind he can hardly stand out against us.\"\"\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompting\n",
    "Zero-shot learning entails the model's ability to produce responses solely based on its pre-existing training data, without any task-specific fine-tuning. Think of it as asking a question without offering any context, relying on the model's knowledge to provide an answer. This approach is applicable to all the examples mentioned above, where the model draws from its general knowledge to respond effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompting\n",
    "Few-shot learning is like equipping someone with a handful of illuminating examples before presenting your ultimate question. This preliminary step enables the model to better grasp and respond to your inquiry with precision. It's akin to laying the groundwork for the model's understanding, similar to how a context-rich conversation aids comprehension.\n",
    "\n",
    "It's crucial to recognize that these \"examples\" need not be confined to the literal sense. You can also attain this effect by retaining prior messages in the model's memory and submitting them along with your latest query. In doing so, past interactions serve as a form of few-shot learning, enriching the context for the AI's responses.\n",
    "\n",
    "However, it's worth considering the size of the Language Model (LLM) at play. Depending on its capacity, you may need to manage its memory by adding a prompt in between to steer the conversation, or just clear the memory all together. This helps maintain clarity and relevance in the ongoing conversation, avoiding any potential misunderstandings or context clutter. \n",
    "\n",
    "To illustrate, if you've been discussing topics related to Homer's Odyssey, but wish to transition to a discussion about modern physics, it might be prudent to clear any remnants of the epic from the model's memory to ensure its responses align with your current subject of interest.\n",
    "\n",
    "Let us try a zero-shot example and see how we can fix it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"I really hated the show!\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown('Zero-Shot: '+response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's evident that without providing context, our LLM tends to give generic answers. However, by furnishing a few preliminary examples or guidelines, you can steer the LLM towards delivering the specific responses you desire. This approach is remarkably versatile and applicable to a wide range of scenarios, from simple queries to complex mathematical formulas, SQL statements, transcript analysis, and beyond.\n",
    "\n",
    "You can amplify this effect by integrating system prompts that instruct the LLM on how to behave. For instance:\n",
    "\n",
    "```markdown\n",
    "system = You're an AI assistant that specializes in analyzing customer support call transcripts to verify if the DPA process was appropriately followed.\n",
    "\n",
    "user: Examples of transcripts and results\n",
    "```\n",
    "\n",
    "By combining user-provided examples with such system prompts, you reinforce the behavior you're seeking from the LLM. This dynamic interaction allows you to extract tailored and context-aware responses, enhancing the model's utility in diverse applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "         {\"role\": \"user\", \"content\": \"I really hated the show! -- negative\"},\n",
    "          {\"role\": \"user\", \"content\": \"I somewhat liked it -- Positive\"},\n",
    "           {\"role\": \"user\", \"content\": \"Meh, i am not really sure about that show -- neutral\"},\n",
    "        {\"role\": \"user\", \"content\": \"It was ok. I guess.. --\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown('Multi-shot: '+response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unveiling Complex Reasoning with Chain of Thought Prompting\n",
    "Chain-of-thought (CoT) prompting is a technique that empowers LLMs with the ability to engage in intricate reasoning by traversing intermediate steps. This approach can be utilized in two ways: as a standalone question (zero-shot) or in combination with few-shot prompting to yield superior results in tasks that demand advanced reasoning before providing a response. It's important to consider the capabilities of the Language Model (LLM) you're working with, as this approach may vary depending on the model's size and capacity. Models like GPT3.5-Turbo or GPT-4 offer substantial capabilities, which can influence the extent to which CoT prompting is necessary.\n",
    "\n",
    "To illustrate the potential of CoT prompting, let's revisit the customer support call transcript scenario mentioned earlier. Rather than simply offering examples, you can delve into the intricate process of determining whether a transcript passes or fails the Data Protection Act (DPA) standards. The level of detail you include in your prompts depends on the LLM's abilities. For instance, a Llama v2 7B model might require more explicit guiding prompts compared to a GPT-4 1.7T model, which can exhibit more advanced reasoning and comprehension capabilities.\n",
    "\n",
    "The key takeaway is that Chain of Thought Prompting is a versatile tool that allows you to harness the full potential of your chosen LLM, optimizing its performance for complex tasks that demand logical reasoning and multi-step decision-making.\n",
    "\n",
    "We firstly look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Zero-Shot\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "Which is a faster way to get to work?\n",
    "Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.\n",
    "Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.\n",
    "\"\"\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown('Zero-Shot: '+response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Few-Shot COT\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"\"\"Which is a faster way to get home?\n",
    "Option 1: Take a 10 minutes bus, then a 40 minute bus, and finally a 10 minute train.\n",
    "Option 2: Take a 90 minutes train, then a 45 minute bike ride, and finally a 10 minute bus.\n",
    "Option 1 will take 10+40+10 = 60 minutes.\n",
    "Option 2 will take 90+45+10 = 145 minutes.\n",
    "Since Option 1 takes 60 minutes and Option 2 takes 145 minutes, Option 1 is faster.\n",
    "\n",
    "Which is a faster way to get to work?\n",
    "Option 1: Take a 1000 minute bus, then a half hour train, and finally a 10 minute bike ride.\n",
    "Option 2: Take an 800 minute bus, then an hour train, and finally a 30 minute bike ride.\n",
    "\"\"\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown('Few-Shot COT: '+response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this technique to a customer support call transcript..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Zero-Shot\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "Transcript: \\n\n",
    "         Agent: Hello, you've reached Virgin Media, can i take your name and postcode please?\\n\n",
    "         Customer: Yes, its Tom Potato and RG30 2ND \\n\n",
    "         Agent: Thank you, how can i help you today? \\n\n",
    "         Customer: I would like to upgrade my account to 1 gigabyte per second please \\n\n",
    "         Agent: That's not a problem. Let me check... Ok, all done!\\n\n",
    "         Customer: Thank you!\\n\n",
    "         Agent: You're very welcome. Goodbye!\\n\n",
    "\n",
    "Task: Analyse the transcript. Is it a DPA pass or fail?\n",
    "\"\"\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown('Zero-Shot: '+response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot COT\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "Transcript: \\n\n",
    "         Agent: Hello, you've reached Virgin Media, can i take your name and postcode please?\\n\n",
    "         Customer: Yes, its Tom Potato and RG30 2ND \\n\n",
    "         Agent: Thank you, how can i help you today? \\n\n",
    "         Customer: I would like to upgrade my account to 1 gigabyte per second please \\n\n",
    "         Agent: That's not a problem. Let me check... Ok, all done!\\n\n",
    "         Customer: Thank you!\\n\n",
    "         Agent: You're very welcome. Goodbye!\\n\n",
    "\n",
    "Task: Analyse the transcript. Is it a DPA pass or fail?\\n\n",
    "Firstly we analyse the transcript to see if the agent found the account, and we found that the agent asked for the name and postcode in order to find the account. \\n\n",
    "Next, we analyse the transcript to see if the agent asked any security questions after finding the account, and we found that no security questions were asked. \\n\n",
    "Therefore, this is a DPA fail because the agent did not ask any security questions.       \n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "Transcript: \\n\n",
    "         Agent: Hello, you've reached Virgin Media, can i take your name and postcode please?\\n\n",
    "         Customer: Yes, its Vanessa Eggplant and OX2 0AU \\n\n",
    "         Agent: Thank you, and for security purposes can i take your date of birth and first line of the address please? \\n\n",
    "         Customer: Yes, its 26/05/1985 and 3 East Street \\n\n",
    "         Agent: Thank you, how can i help you today? \\n\n",
    "         Customer: I would like to upgrade my account to 1 gigabyte per second please \\n\n",
    "         Agent: That's not a problem. Let me check... Ok, all done!\\n\n",
    "         Customer: Thank you!\\n\n",
    "         Agent: You're very welcome. Goodbye!\\n\n",
    "\n",
    "Task: Analyse the transcript. Is it a DPA pass or fail?\\n\n",
    "Firstly we analyse the transcript to see if the agent found the account, and we found that the agent asked for the name and postcode in order to find the account. \\n\n",
    "Next, we analyse the transcript to see if the agent asked any security questions after finding the account, and we found that the agent asked for the date of birth and the first line of the address, which count as two DPA questions. \\n\n",
    "Therefore, this is a DPA pass because the agent asked two security questions after finding the account.\n",
    "         \n",
    "         \n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "Transcript: \\n\n",
    "         Agent: Hello, you've reached Virgin Media, can i take your name and postcode please?\\n\n",
    "         Customer: Yes, its Ada Lovelace and NG17 8BY \\n\n",
    "         Agent: Thank you, how can i help you today? \\n\n",
    "         Customer: I would like to upgrade my account to 1 gigabyte per second please \\n\n",
    "         Agent: That's not a problem. Let me check... Ok, all done!\\n\n",
    "         Customer: Thank you!\\n\n",
    "         Agent: You're very welcome. Goodbye!\\n\n",
    "     Task: Analyse the transcript. Is it a DPA pass or fail? \n",
    "\"\"\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown('Multi-Shot: '+response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elevating Business Value: Effective Prompt Engineering and Tuning\n",
    "The distinction between a fundamental chatbot and a business-enabling application lies in the adept utilization of prompt engineering and fine-tuning. To exemplify the significance of fine-tuning, let's explore an alternate scenario. Picture this as a component of an application you've meticulously crafted, aiming to showcase it with precision.\n",
    "\n",
    "Within this application, the LLM can be directed to provide results in formats that seamlessly integrate into your internal processes. This adaptability extends to the generation of straightforward true/false statements or structured data in JSON format, which can then be seamlessly integrated into downstream operations. This level of control and customization ensures that the LLM aligns with your specific business needs, enabling you to harness its potential in a way that maximizes operational efficiency and value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\":\"system\",\"content\":\"You only provide answers in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "Transcript: \\n\n",
    "         Agent: Hello, you've reached Virgin Media, can i take your name and postcode please?\\n\n",
    "         Customer: Yes, its Tom Potato and RG30 2ND \\n\n",
    "         Agent: Thank you, how can i help you today? \\n\n",
    "         Customer: I would like to upgrade my account to 1 gigabyte per second please \\n\n",
    "         Agent: That's not a problem. Let me check... Ok, all done!\\n\n",
    "         Customer: Thank you!\\n\n",
    "         Agent: You're very welcome. Goodbye!\\n\n",
    "\n",
    "Task: Analyse the transcript. Is it a DPA pass or fail?\\n\n",
    "Firstly we analyse the transcript to see if the agent found the account, and we found that the agent asked for the name and postcode in order to find the account. \\n\n",
    "Next, we analyse the transcript to see if the agent asked any security questions after finding the account, and we found that no security questions were asked. \\n\n",
    "Therefore, this is a DPA fail because the agent did not ask any security questions. \\n\n",
    "{\"result\":\"fail\",\"reasoning\":\"This is a DPA fail because the agent did not ask any security questions after finding the account\"}\n",
    "         \n",
    "         \n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "Transcript: \\n\n",
    "         Agent: Hello, you've reached Virgin Media, can i take your name and postcode please?\\n\n",
    "         Customer: Yes, its Vanessa Eggplant and OX2 0AU \\n\n",
    "         Agent: Thank you, and for security purposes can i take your date of birth and first line of the address please? \\n\n",
    "         Customer: Yes, its 26/05/1985 and 3 East Street \\n\n",
    "         Agent: Thank you, how can i help you today? \\n\n",
    "         Customer: I would like to upgrade my account to 1 gigabyte per second please \\n\n",
    "         Agent: That's not a problem. Let me check... Ok, all done!\\n\n",
    "         Customer: Thank you!\\n\n",
    "         Agent: You're very welcome. Goodbye!\\n\n",
    "\n",
    "Task: Analyse the transcript. Is it a DPA pass or fail?\\n\n",
    "Firstly we analyse the transcript to see if the agent found the account, and we found that the agent asked for the name and postcode in order to find the account. \\n\n",
    "Next, we analyse the transcript to see if the agent asked any security questions after finding the account, and we found that the agent asked for the date of birth and the first line of the address, which count as two DPA questions. \\n\n",
    "Therefore, this is a DPA pass because the agent asked two security questions after finding the account.\\n\n",
    "{\"result\":\"pass\",\"reasoning\":\"This is a DPA pass. The agent asked two DPA questions after finding the account\"} \n",
    "         \n",
    "         \n",
    "\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "Transcript: \\n\n",
    "         Agent: Hello, you've reached Virgin Media, can i take your name and postcode please?\\n\n",
    "         Customer: Yes, its Ada Lovelace and NG17 8BY \\n\n",
    "         Agent: Thank you, how can i help you today? \\n\n",
    "         Customer: I would like to upgrade my account to 1 gigabyte per second please \\n\n",
    "         Agent: That's not a problem. Let me check... Ok, all done!\\n\n",
    "         Customer: Thank you!\\n\n",
    "         Agent: You're very welcome. Goodbye!\\n\n",
    "     Your tasks are as follows:\\n\n",
    "     1. Analyse the transcript. Is it a DPA pass or fail?\\n\n",
    "     2. Summarise the transcript\\n\n",
    "     3. Extract the customer's name\n",
    "     4. Provide the overall sentiment of the transcript. \n",
    "     5. Was the agent polite and professional throughout the call?\n",
    "\n",
    "     provide a JSON output with the following keys:\n",
    "\n",
    "     dpa_result,dpa_reasoning,summary,cust_name,sentiment,politeness\n",
    "\"\"\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "results = []\n",
    "\n",
    "raw_output = response.choices[0].message.content\n",
    "\n",
    "print (raw_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't it remarkable? However, the world of customer support calls is far from straightforward, and achieving precise results often requires an intricate web of prompts. Changing the structure or language of these prompts can lead to varying degrees of success, heavily influenced by the size of the LLM in use. To navigate this complexity, a strategic blend of COT with both system and user prompts is recommended. This combination establishes the desired behavior and raison d'etre, which is then further reinforced through illustrative examples.\n",
    "\n",
    "It's vital to note that these prompts are effective in both inference and fine-tuning phases. In instances where your prompts may not yield the desired results, and adding more examples is a costly endeavor in terms of token consumption, you have the option to consolidate these few-shot examples into a JSONL file for more effective fine-tuning.\n",
    "\n",
    "Moreover, to mitigate the potential for hallucinations, always include a clear prompt explaining how the LLM should respond when it encounters a situation where no DPA-related questions are found. This precaution is essential, as LLMs are word prediction models and might invent responses if not explicitly guided.\n",
    "\n",
    "When experimenting with different transcripts, feel free to submit multiple queries simultaneously, provided they don't necessitate their own unique COT processes. However, the approach should be tailored to the LLM's size and capabilities. If it's more efficient to split tasks, save the results in a JSON repository, and then present them, that's a viable strategy to optimize performance.\n",
    "\n",
    "As you add more behavioral prompts and procedures, you might encounter instances where the LLM doesn't respond as expected. Here are some unconventional tips to fine-tune its performance:\n",
    "\n",
    "- **Identity Reinforcement:** Assign a distinct identity (be it X, potato, or any other moniker) and refer to it during your interactions. This helps reinforce the grounding prompts without reiterating them.\n",
    "- **Politeness:** Surprisingly, courtesy can go a long way in obtaining better responses.\n",
    "- **Allow Time:** Encourage the LLM to take a moment to process complex tasks, as it can lead to more accurate and thoughtful outputs.\n",
    "\n",
    "Now, let's adapt the example above by eliminating all few-shot examples and leveraging Zero-Shot Chain of Thought with identity prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity Prompts plus Zero-SHOT COT\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\":\"system\", \"content\":\"Your name is X, you're an AI assistant that is specialised in supporting customer support quality assurance.\"},\n",
    "        {\"role\":\"system\", \"content\":\"As X, you analyse customer support transcripts to see if DPA security questions were asked by the agent. Providing a pass or fail as appropriate\"},\n",
    "        {\"role\":\"system\", \"content\":\"As X, you do this by firstly analysing the transcript to see if the agent found the account using the name and postcode, after that you see if any security questions were asked.\"},\n",
    "        {\"role\":\"system\",\"content\":\"As X, You only provide answers in JSON format.\"},\n",
    "     \n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "Transcript: \\n\n",
    "         Agent: Hello, you've reached Virgin Media, can i take your name and postcode please?\\n\n",
    "         Customer: Yes, it's Ada Lovelace and NG17 8BY \\n\n",
    "         Agent: Thank you, how can i help you today? \\n\n",
    "         Customer: I would like to upgrade my account to 1 gigabyte per second please \\n\n",
    "         Agent: That's not a problem. Let me check... Ok, all done!\\n\n",
    "         Customer: Thank you!\\n\n",
    "         Agent: You're very welcome. Goodbye!\\n\n",
    "     Hello X, please take a deep breath and analyse the transcript. Your tasks are as follows:\\n\n",
    "     1. Is it a DPA pass or fail?\\n\n",
    "     2. Summarise the transcript\\n\n",
    "     3. Extract the customer's name\\n\n",
    "     4. Provide the overall sentiment of the transcript.\\n\n",
    "     5. Was the agent polite and professional throughout the call?\\n\n",
    "\n",
    "     provide a JSON output with the following keys:\n",
    "     dpa_result,dpa_reasoning,summary,cust_name,sentiment,politeness\n",
    "\"\"\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "results = []\n",
    "\n",
    "raw_output = response.choices[0].message.content\n",
    "\n",
    "print (raw_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive, Right? I trust this explanation has provided clarity and sparked your curiosity to explore a diverse range of application use-cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
