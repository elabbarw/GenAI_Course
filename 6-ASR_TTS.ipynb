{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Communication: Integrating Voice with Language Models\n",
    "\n",
    "Understanding language models has opened new horizons for us, but let's elevate our interaction with technology by incorporating voice functionalities. Voice technology not only makes digital content more accessible but also enhances the user experience. By integrating Automatic Speech Recognition (ASR) and Text to Speech (TTS) systems, we can create more natural and engaging interfaces.\n",
    "\n",
    "Let's delve into some practical applications:\n",
    "\n",
    "1. **Education**: ASR can transcribe lectures in real-time, aiding students who prefer reading over listening or those with hearing impairments. Similarly, TTS can read aloud text materials for visually impaired students or for those who learn better through auditory means.\n",
    "\n",
    "2. **Customer Service**: Implementing voice in chatbots can lead to more interactive and human-like customer service experiences. It allows users to receive assistance through voice commands, making technology more accessible for everyone, including those with disabilities or the elderly.\n",
    "\n",
    "3. **Healthcare**: TTS can assist patients by reading out health information, while ASR can help doctors transcribe notes hands-free, increasing efficiency and allowing more time for patient care.\n",
    "\n",
    "4. **Multilingual Interactions**: Voice technology can break language barriers. With ASR and TTS, we can develop systems that understand and speak multiple languages, helping in situations like tourism and international business.\n",
    "\n",
    "Here are some coding examples to help you integrate these technologies into your projects. Once you're familiar with the basics, why not experiment with the Streamlit chatbots I've set up? They offer a sandbox for you to test and refine voice-enabled applications. This hands-on experience will not only solidify your understanding but also inspire innovation in voice technology applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Whisper: A Leap in Speech Processing\n",
    "\n",
    "OpenAI Whisper represents a significant advancement in the realm of speech processing technologies. This model is meticulously trained on extensive audio datasets to deliver accurate transcription and translation services. Whisper's dual capabilities make it a versatile tool for various applications, from real-time captioning to multilingual communication support.\n",
    "\n",
    "Whisper is accessible in two main forms:\n",
    "\n",
    "1. **API Service**: OpenAI provides Whisper as an API, allowing developers to easily integrate its functionalities into their applications. This option is ideal for those seeking a quick and efficient way to implement speech recognition without the complexities of model training and maintenance.\n",
    "\n",
    "2. **Open-Source Model**: In a move towards openness and collaboration, Whisper has also been made available as an open-source model. This is particularly exciting for the developer community as it opens up possibilities for customization and improvement. Anyone with the technical know-how can download, modify, and utilize the model to fit their specific needs.\n",
    "\n",
    "The open-source aspect encourages experimentation and innovation, offering a hands-on approach for those who wish to delve deeper into the inner workings of the model. Whether it's for educational purposes, research, or developing bespoke applications, Whisper's open-source availability is a boon for creators and innovators worldwide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install elevenlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "from openai import OpenAI\n",
    "import IPython\n",
    "\n",
    "\n",
    "# Set the client and api key\n",
    "\n",
    "llm_client = OpenAI(\n",
    "    api_key=''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_file = open('./media/irish_schoolboy_frostbite.mp3','rb')  ### Whisper can handle this accent quite accurately :p\n",
    "\n",
    "transcription = llm_client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=media_file,\n",
    "    language='en' ### Important to get better quality, especially with *challenging* accents. Otherwise, you end up with Welsh instead of English, etc..\n",
    "    )\n",
    "\n",
    "print (transcription.text)\n",
    "IPython.display.Audio(\"./media/irish_schoolboy_frostbite.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation into English:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jap_file = open('./media/japanese.mp3','rb')\n",
    "\n",
    "translation = llm_client.audio.translations.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=jap_file\n",
    "    )\n",
    "\n",
    "print (translation.text)\n",
    "IPython.display.Audio(\"./media/japanese.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring OpenAI's Text to Speech Innovations\n",
    "\n",
    "OpenAI's recent foray into Text to Speech (TTS) technology has resulted in advanced models that are reshaping our expectations of synthetic voice quality. These models have made strides in creating voices that are more lifelike and expressive than ever before, surpassing older models such as those provided by Google.\n",
    "\n",
    "Key features of OpenAI's TTS models include:\n",
    "\n",
    "1. **Naturalness**: The voices produced by these models are not just clear but also exhibit a degree of natural inflection and rhythm that closely mimics human speech.\n",
    "\n",
    "2. **Expressiveness**: Unlike earlier TTS systems, which often sounded monotone, these models can convey emotions and emphasis, enhancing the listening experience.\n",
    "\n",
    "3. **Customization**: These models can be tailored to different use cases, providing varying tones and styles suitable for a range of applications, from audiobooks to virtual assistants.\n",
    "\n",
    "\n",
    "Whether you are a developer looking to incorporate voice into your app, an educator creating more engaging learning materials, or a content creator seeking to produce high-quality audio content, OpenAI's TTS models can serve as a cornerstone technology.\n",
    "\n",
    "To experiment with these models, one could start by generating voice samples from text scripts or integrating the TTS API into existing applications to see firsthand the improvements in voice synthesis. These practical trials not only demonstrate the capability of the technology but also inspire innovative uses of TTS in various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = '''Tell me, O Muse, of that ingenious hero who travelled far and wide\n",
    "after he had sacked the famous town of Troy.'''\n",
    "\n",
    "TEMP_AUDIOFILENAME = 'audio_temp.mp3'\n",
    "\n",
    "tts = llm_client.audio.speech.create(\n",
    "                    model='tts-1',\n",
    "                    voice='fable',\n",
    "                    input=Text\n",
    "                )\n",
    "tts.stream_to_file(TEMP_AUDIOFILENAME) ### Saves to file, you can either consume directly or open it and run it within the app (check the chatbots)\n",
    "IPython.display.Audio(TEMP_AUDIOFILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try something even better! Elevenlabs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import generate\n",
    "\n",
    "audio = generate(\n",
    "  text=\"Hello! 你好! Hola! नमस्ते! Bonjour! こんにちは! مرحبا! 안녕하세요! Ciao! Cześć! Привіт! வணக்கம்!\",\n",
    "  voice=\"Bella\",\n",
    "  model=\"eleven_multilingual_v2\"\n",
    ")\n",
    "\n",
    "IPython.display.Audio(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import generate, set_api_key\n",
    "\n",
    "set_api_key(\"\") # Get api key from elevenlabs\n",
    "\n",
    "audio = generate(\n",
    "  text=\"\"\"\n",
    "  دَعِ الأَيَّامَ تَفْعَل مَا تَشَاءُ\n",
    "  .\n",
    "  .\n",
    "  وطب نفساً إذا حكمَ القضاءُ\n",
    "  .\n",
    "  .\n",
    "  وَلا تَجْزَعْ لنازلة الليالي\n",
    "  .\n",
    "  .\n",
    "  فما لحوادثِ الدنيا بقاءُ \n",
    "  .\n",
    "  .\n",
    "  وكنْ رجلاً على الأهوالِ جلداً \n",
    "  .\n",
    "  .\n",
    "  وشيمتكَ السماحة ُ والوفاءُ \n",
    "  .\n",
    "  .\n",
    "  وإنْ كثرتْ عيوبكَ في البرايا \n",
    "  .\n",
    "  .\n",
    "  وسَركَ أَنْ يَكُونَ لَها غِطَاءُ\n",
    "  .\n",
    "  .\n",
    "  تَسَتَّرْ بِالسَّخَاء فَكُلُّ عَيْب\n",
    "  .\n",
    "  .\n",
    "  يغطيه كما قيلَ السَّخاءُ\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \"\"\",\n",
    "  voice=\"Daniel\",\n",
    "  model=\"eleven_multilingual_v1\"\n",
    ")\n",
    "\n",
    "IPython.display.Audio(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The potential applications for integrating language models with voice technologies like ASR and TTS are indeed extensive and exciting. With the advanced capabilities provided by models like OpenAI's Whisper for ASR and the latest TTS systems, developers and innovators can create interactive and responsive chatbots that elevate user experience across various platforms.\n",
    "\n",
    "Let's explore the possibilities:\n",
    "\n",
    "Web Applications: Enhance user engagement by embedding voice-enabled chatbots on websites. This can assist users in navigating the site, provide instant customer support, and facilitate accessibility for those with visual impairments.\n",
    "\n",
    "Smartphone Apps: Integrate voice commands and responses into mobile applications, making them hands-free and more convenient for users on the go. This could be particularly useful in apps for smart home control, personal assistants, and language learning.\n",
    "\n",
    "Robotics: In robotics, voice interaction can make robots more user-friendly and capable of performing complex tasks through simple voice commands. This has profound implications for personal assistance robots, educational robots, and those used in customer service.\n",
    "\n",
    "Voice-Activated Devices: Devices similar to Alexa can be enhanced with OpenAI's LLM as a skill, incorporating both ASR and TTS. This would allow for more nuanced understanding and responses, making interactions with these devices more natural and efficient.\n",
    "\n",
    "Accessibility Tools: For individuals with disabilities, voice technologies can be life-changing. They can control technology, access information, and communicate with others without the need for traditional input methods.\n",
    "\n",
    "Automotive Systems: Voice-enabled systems in vehicles can provide a safer and more intuitive way for drivers to control their environment, access navigation, and communicate without taking their hands off the wheel.\n",
    "\n",
    "The convergence of language models with voice technologies is not just about convenience; it’s about creating inclusive and innovative experiences that were not possible before. Whether it's through play and experimentation or structured development, the potential to revolutionize how we interact with machines is at our fingertips. Encouraging experimentation with chatbots and other voice-enabled applications not only demonstrates the versatility of these technologies but also sparks creativity in finding new solutions and applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
