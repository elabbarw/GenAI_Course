{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unleashing the Potential of Vector Databases: Amplifying RAG!\n",
    "\n",
    "Ever pondered on how to elevate the capabilities of your Retrieval Augmented Generation (RAG) with the prowess of a Vector Database? Well, you're in for a treat! We're about to embark on a journey to establish a VectorDB using Qdrant (https://cloud.qdrant.io/login), and the best part is that they provide 1GB of free usage.\n",
    "\n",
    "Here's the roadmap:\n",
    "\n",
    "1. Start by signing up for an account on Qdrant.\n",
    "2. Navigate to the \"Clusters\" section and create one. Ensure that you remain within the 1GB limit to enjoy the benefits of the free tier.\n",
    "3. Upon cluster creation, you'll receive a message indicating \"no API keys created for this cluster.\" Click on \"API keys\" to commence their generation.\n",
    "4. In the API keys section, you'll discover the API key itself and a collection of code samples in various programming languages for database connectivity. For our purposes, we'll utilize the Python code for connection.\n",
    "\n",
    "By following this path, you'll harness the capabilities of a Vector Database, supercharging your RAG endeavors and opening the door to a world of enhanced responses and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have had to decrease the amount of text sent to ADA in order to account for the limits imposed on free users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install qdrant-client\n",
    "%pip install gptrim\n",
    "%pip install tiktoken\n",
    "%pip install langchain\n",
    "%pip install gptrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import our modules\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.http.models import PointStruct\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import csv\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import ast\n",
    "from gptrim import trim\n",
    "import json\n",
    "\n",
    "\n",
    "# Load the vectordb with your url and apikey\n",
    "qdrant_client = QdrantClient(\n",
    "    url='', \n",
    "    api_key='',\n",
    ")\n",
    "\n",
    "\n",
    "# Set the client and api key\n",
    "\n",
    "llm_client = OpenAI(\n",
    "    api_key=''\n",
    ")\n",
    "# Let's set our model\n",
    "MODEL = \"gpt-3.5-turbo\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process outlined above loads our vector database into memory, making it ready for subsequent queries and imports. Now, let's tackle the task of importing a substantial text, like Homer's Odyssey, into a vector database while addressing some key challenges:\n",
    "\n",
    "## Challenge 1 - Document Vectorization\n",
    "\n",
    "### Document Vectorization Demystified\n",
    "Document vectorization involves the conversion of textual documents into numerical vectors. These vectors typically encapsulate the semantic meaning and content of the documents. Various techniques exist for document vectorization, ranging from traditional Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) to modern approaches like Word2Vec and Doc2Vec embeddings.\n",
    "\n",
    "### Significance of Document Vectorization Before Vector Database Import\n",
    "\n",
    "- **Semantic Search:** Vector databases are tailored for similarity searches within high-dimensional vector spaces. Vectorizing documents equips them for semantic content comparison, enabling the prowess of semantic search.\n",
    "- **Efficiency:** Text data is inherently unstructured and can vary significantly in length. By transforming text into fixed-length vectors, it becomes more manageable, storable, and searchable.\n",
    "- **Comparability:** Representing documents as vectors allows for the measurement of distance or similarity between them using metrics like cosine similarity. This is indispensable for tasks such as document clustering, recommendation systems, and classification.\n",
    "- **Dimensionality Reduction:** Techniques like TF-IDF or embeddings often reduce the dimensionality of the data by focusing on the most informative features. This results in more efficient storage and faster search.\n",
    "- **Machine Learning Compatibility:** Numerous machine learning algorithms demand numerical input. Vectorized documents can be directly fed into these algorithms, facilitating tasks such as topic modeling, sentiment analysis, and more.\n",
    "\n",
    "By addressing these considerations, we not only enhance the utility of our vector database but also open up possibilities for advanced search, retrieval, and machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While QDRANT offers a straightforward CPU-based method for document vectorization, I will illustrate an approach to vectorize documents using OpenAI's ADA Language Model, keeping in mind that you may be developing your own vectorization methods using APIs or GPUs. Relying solely on the CPU, especially in the present context, might not be sufficient. This is because you'll need to vectorize both the documents and the user-submitted queries, which can be time-consuming. In contrast, you'd likely prefer instantaneous responses from your chatbot rather than waiting for extended processing times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load the book (three books from the odyssey)\n",
    "\n",
    "with open('./books/theodyssey.txt', 'r', encoding='UTF-8') as file:\n",
    "    book = file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2 - Addressing Text Length Limitations\n",
    "\n",
    "Hold on a moment, this is quite an extensive amount of text! It's apparent that we won't be inputting all of this in a single prompt.\n",
    "\n",
    "As we embark on the journey of document vectorization, it's imperative to grasp that whatever the Vector Database (DB) returns, whether condensed or not, will be combined with your question and any other historical prompts if you're maintaining a memory. Consequently, if you opt to house entire documents or books within a VectorDB, you're bound to encounter the constraints of maximum token limits imposed by your API. This, in turn, may lead to escalating costs, even for straightforward inquiries.\n",
    "\n",
    "So, how do we surmount this challenge? The solution lies in segmenting our documents into smaller text snippets before commencing the vectorization process.\n",
    "\n",
    "Let's press on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tokens would this book require? Let us use tiktoken to find out...\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "numtokens = len(encoding.encode(book))\n",
    "\n",
    "gpt3cost = round((numtokens / 1000) * 0.0015, 2)\n",
    "gpt316kcost = round((numtokens / 1000) * 0.003, 2)\n",
    "gpt4cost = round((numtokens / 1000 ) * 0.03, 2)\n",
    "gpt432kcost = round((numtokens / 1000) * 0.06, 2)\n",
    "\n",
    "adacost = round((numtokens / 1000) * 0.0001, 10)\n",
    "\n",
    "\n",
    "print (f'This book without any processing is:\\n{numtokens} Tokens using GPT3.5 encoding')\n",
    "\n",
    "\n",
    "print (f'This will cost a total of:\\n${gpt3cost} GPT3.5 \\n${gpt316kcost} GPT3.5 16K \\n${gpt4cost} GPT4 \\n${gpt432kcost} GPT4 32K')\n",
    "\n",
    "\n",
    "print (f'Vectorising this book using OpenAI ADA will cost: ${adacost}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's evident that submitting the entire book as a prompt won't be feasible, given the token limits of GPT-3.5, which is 4000 or 16,000 tokens, and GPT-4, which is 8000 tokens. Therefore, let's explore the approach of breaking down our book into smaller text snippets. We have at our disposal various Langchain classes for this purpose, and for this task, we will make use of the \"RecursiveCharacterTextSplitter\" class from the Langchain library. This tool excels in segmenting text into smaller chunks based on character limits, making it a valuable asset for dividing lengthy content into manageable portions that can be processed by a language model like GPT-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have had to increase chunk size in order to fix the issue with the free tier of ADA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let us set our text splitter to use tiktoken encoder as a way to split our text\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1200, chunk_overlap=120\n",
    ")\n",
    "\n",
    "# create seperate text snippets from the book as a list\n",
    "split_book = text_splitter.split_text(book)\n",
    "\n",
    "print (f'Number of chunks : {len(split_book)}') # number of chunks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = []\n",
    "for snippet in split_book:\n",
    "    item = {\n",
    "        'source': \"Homer's Odyssey\",\n",
    "        'text': snippet\n",
    "    }\n",
    "\n",
    "    documents.append(item)\n",
    "\n",
    "\n",
    "print (documents[1]) # We have document snippets to import now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save as CSV for analysis and import later on.\n",
    "\n",
    "# Define the fields for your CSV\n",
    "fields = ['source', 'text']\n",
    "\n",
    "# Name of the output csv file\n",
    "filename = \"documents.csv\"\n",
    "\n",
    "# Writing to csv file\n",
    "with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Create a csv dict writer object\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "\n",
    "    # Write the headers\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data\n",
    "    writer.writerows(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've compiled our list of documents, composed of text snippets and saved as a CSV file, let's shift our focus to the process of vectorization. We'll be generating text embeddings and preserving the results in a CSV file, ensuring that we don't need to call ADA again.\n",
    "\n",
    "# Understanding Embeddings\n",
    "Working with textual data presents a challenge because our computers, scripts, and machine learning models lack the capacity to read and comprehend text in a human sense. To bridge this gap, we employ a technique known as \"embedding\" to represent data in the form of numerical vectors in a higher-dimensional space. In the context of semantic search, embeddings are crafted using deep learning models, encompassing word embeddings (e.g., Word2Vec, GloVe) for natural language data and image embeddings for visual content.\n",
    "\n",
    "OpenAI's text embeddings play a pivotal role in gauging the relatedness of text strings. Embeddings find widespread application in various tasks, including:\n",
    "\n",
    "- Search, where results are sorted based on their relevance to a query string.\n",
    "- Clustering, where text strings are grouped according to their similarity.\n",
    "- Recommendations, where items with closely related text strings are suggested.\n",
    "- Anomaly detection, which identifies outliers with minimal relatedness.\n",
    "- Diversity measurement, involving the analysis of similarity distributions.\n",
    "- Classification, where text strings are categorized based on their closest matching label.\n",
    "\n",
    "An embedding essentially consists of a vector (list) of floating-point numbers. The proximity between two vectors serves as a gauge of their relatedness, with shorter distances indicating higher relatedness and longer distances implying lower relatedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "input_datapath = \"./documents.csv\"\n",
    "output_datapath = \"documents_embeddings.csv\"\n",
    "\n",
    "# Prepare a list to store the data\n",
    "documents = []\n",
    "\n",
    "with open(input_datapath, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        if row[\"source\"] and row[\"text\"]:  # Checking if both fields are not empty\n",
    "            combined = \"Source: \" + row[\"source\"].strip() + \"; Text: \" + row[\"text\"].strip()\n",
    "            documents.append({\n",
    "                \"text\": combined\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippet below will start the embedding process with ADA. This will take some time to complete. I have also added a counter to avoid ADA's api limits for the free tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "# Counter to avoid rate limits with ADA (assuming free tier)\n",
    "rate_limit_counter_free = 0\n",
    "\n",
    "# Generate embeddings and add as a seperate column\n",
    "for doc in documents:\n",
    "    rate_limit_counter_free += 1\n",
    "    if rate_limit_counter_free == 3:\n",
    "        time.sleep(60)\n",
    "        rate_limit_counter_free = 0\n",
    "    doc['embedding'] = llm_client.embeddings.create(\n",
    "        input = doc['text'],\n",
    "        model = embedding_model\n",
    "    ).data[0].embedding\n",
    "\n",
    "# Save results to a CSV file\n",
    "with open(output_datapath, 'w', encoding='utf-8') as file:\n",
    "    fieldnames = [\"text\", \"embedding\"]\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for doc in documents:\n",
    "        writer.writerow(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine both the \"documents.csv\" file and the one containing embeddings. We are now prepared to utilize these embeddings for importing into QDRANT.\n",
    "\n",
    "This process needs to be meticulously repeated for each individual document or book that you wish to import, as well as for incoming questions, which we'll address shortly. Picture having a nightly task in place to import data from Confluence, conduct the necessary splitting, generate embeddings, and import them into the database. This ensures that your chatbot can furnish appropriate responses when a knowledge base (KB) skill is triggered.\n",
    "\n",
    "At this juncture, I will proceed to establish a collection in QDRANT, denominated as 'Books,' and delineate the parameters for importing documents. Visualize this as akin to configuring a conventional SQL table with distinct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us create our collection and call it 'Books'\n",
    "collection_name = 'books'\n",
    "\n",
    "qdrant_client.recreate_collection(\n",
    "    collection_name=f\"{collection_name}\",\n",
    "    vectors_config=models.VectorParams(\n",
    "      size=1536,\n",
    "      distance=models.Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "## Let's check out our collection\n",
    "\n",
    "collection_info = qdrant_client.get_collection(collection_name=\"books\")\n",
    "\n",
    "print(\"Collection info:\", collection_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your collection now in place, head over to your QDRANT cluster's dashboard (don't forget to use your API key) to view the newly created collection.\n",
    "\n",
    "Now, let's proceed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Homer's Odyssey into QDRANT\n",
    "\n",
    "\n",
    "## We will build a new list using the PointStruct class and use it to import into qdrant\n",
    "points = []\n",
    "\n",
    "## We will use a counter to generate ids\n",
    "i = 1\n",
    "\n",
    "## Load our saved CSV file...\n",
    "with open('./documents_embeddings.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        ## Increment the counter\n",
    "        i += 1\n",
    "\n",
    "        ## We will need to use ast in order to bring back the list of vectors since it is represented as string in the CSV. You won't have this problem if you load embeddings directly\n",
    "        ## But i wanted you to have the CSV to take a look at the data.\n",
    "        embeddings = ast.literal_eval(row['embedding'])\n",
    "\n",
    "        # Use PointStruct to append to the list above\n",
    "        points.append(PointStruct(\n",
    "            id=i,  # Our ID\n",
    "            vector=embeddings, # Our embeddings \n",
    "            payload={\"text\": row['text']}) # Our payload which will contain the actual text.\n",
    "            )\n",
    "        \n",
    "\n",
    "### LETS IMPORT\n",
    "\n",
    "operation_info = qdrant_client.upsert(\n",
    "    collection_name=\"books\",\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "print(\"Operation info:\", operation_info)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to breathe. We've successfully set up a vector database with data ready for searching. Return to your database's dashboard and select the collection to explore its contents.\n",
    "\n",
    "Now, let's delve into QDRANT's query functions. Keep in mind that we still need to vectorize the query for this to work seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"Why is Neptune still furious with Ulysses?\"\n",
    "\n",
    "\n",
    "\n",
    "response = llm_client.embeddings.create(\n",
    "        input=QUERY,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "\n",
    "embeddings = response.data[0].embedding\n",
    "\n",
    "print (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = qdrant_client.search(\n",
    "    collection_name=\"books\",\n",
    "    query_vector=embeddings, \n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(search_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can probably discern, the output consists of the five closest text snippets to the query I submitted. These snippets are now ready to be presented to a Language Model for the generation of a suitable response.\n",
    "\n",
    "Shall we proceed with encapsulating this in a skill? Let's go for it!\n",
    "\n",
    "Similar to the internet search skill, we will craft a function tailored for knowledge base search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_kb(query):\n",
    "\n",
    "    response = llm_client.embeddings.create(\n",
    "            input=query,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "\n",
    "    embeddings = response.data[0].embedding\n",
    "\n",
    "    search_result = str(qdrant_client.search(\n",
    "        collection_name=\"books\",\n",
    "        query_vector=embeddings, \n",
    "        limit=3\n",
    "    )\n",
    ")\n",
    "    search_prompt = f\"\"\"\n",
    "        Based on the knowledge base snippets provided in <>, provide an answer to the query [] if it is relevant along with a source. \\\n",
    "        If there are no snippets or if they are not relevant then say \\\"Please try again.\\\"\\\n",
    "        \n",
    "        context:<{trim(search_result)}>\n",
    "        query:[{query}]\n",
    "        \"\"\"\n",
    "    \n",
    "    return {\"role\":\"system\",\"content\": search_prompt}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Requires a function \n",
    "QUESTION = \"Search the knowledge base for why is Neptune still furious with Ulysses?\"\n",
    "## Doesn't require a function\n",
    "#QUESTION = \"How can i make a cup of tea?\"\n",
    "\n",
    "\n",
    "# Let us define the search_internet function for the LLM...\n",
    "skill_definitions = [\n",
    "      {\n",
    "        \"name\": \"search_kb\",\n",
    "        \"description\": \"Searches the knowledge base to provide context to a query if required\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The query to search for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You have tools that add more up to date context to queries should they be required, otherwise act as a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": QUESTION}\n",
    "    ]\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=skill_definitions, # we present our skill definitions to the LLM. Think of it as a toolbox of functions :)\n",
    "    function_call='auto' # auto means that we let the LLM decide when to use a function. You can also set a static function name here so that it will always call it.\n",
    ")\n",
    "### Let us take the JSON response and load it\n",
    "\n",
    "\n",
    "### Do we need a skill?\n",
    "if 'function_call' in response.choices[0].finish_reason:\n",
    "\n",
    "    # tie response with our functions\n",
    "    available_functions = {\n",
    "        'search_kb': search_kb\n",
    "    }\n",
    "\n",
    "\n",
    "    # extract function name and arguments from the LLM\n",
    "    function_name = response.choices[0].message.function_call.name\n",
    "    function_args = json.loads(response.choices[0].message.function_call.arguments)\n",
    "\n",
    "    function2call = available_functions.get(function_name)\n",
    "\n",
    "    # call it appropriately\n",
    "    function_response = function2call(**function_args)\n",
    "\n",
    "    # append the response to the message history\n",
    "    messages.append(function_response)\n",
    "\n",
    "    # send the message history to the LLM again without functions.\n",
    "    new_response = llm_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    #functions=skill_definitions,\n",
    "    #function_call='auto' \n",
    ")\n",
    "    \n",
    "    # Result\n",
    "\n",
    "    print ('\\n\\n'+str(new_response.choices[0].message.content))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(response.choices[0].message.content) \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it. We've successfully deployed a Vector Database, performed document and question vectorization, and created a Question-Answer (QA) chatbot that responds to triggers effectively :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food for Thought\n",
    "\n",
    "RAG and VectorDB aren't limited to knowledge base (KB) searches of documents. They can also serve various other purposes:\n",
    "\n",
    "- Instead of storing numerous prompts for different procedures or relying on Python conditionals to select specific prompts, you can establish a permanent repository for company policies. For instance, in the context of customer service QA scoring, where different procedures are followed based on the initial query, you can configure your function calls to consistently access the \"company_policy\" function. This way, you can dynamically enhance your prompts in response to the specific inquiry (e.g., retrieve Data Protection Act (DPA) policies, create a system prompt, add to memory, and submit).\n",
    "- You can employ RAG and VectorDB for the enduring storage and retrieval of user interaction history. Summarize the interactions with the LLM or preserve them as they are and store them in the database. This enables the LLM to recollect past conversations, fostering a deeper understanding and context in future interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
