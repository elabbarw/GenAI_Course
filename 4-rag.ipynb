{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empowering Chatbots with Retrieval Augmented Generation (RAG)!\n",
    "\n",
    "Imagine supercharging your chatbot with the ability to swiftly comb through the vast resources of the internet or specific databases, enabling it to provide more up-to-date and insightful answers. This is where the magic of RAG comes into play!\n",
    "\n",
    "Here's the scoop: When a user poses a question to your chatbot, instead of solely relying on its internal knowledge, the chatbot sends the query to an API, such as an Internet Search or VectorDB. It then leverages the results to construct a more comprehensive and informed response. Think of it as your chatbot conducting quick online research before delivering an answer!\n",
    "\n",
    "## What Makes RAG So Remarkable?\n",
    "\n",
    "- Cost-Effective: Unlike the resource-intensive process of fine-tuning, RAG is more lightweight and easier to manage.\n",
    "- Human Analogy: Consider your information needs. Would you attempt to absorb the entirety of a library's content, or would you locate the precise book and extract what you require? RAG operates on the latter principle. \n",
    "\n",
    "For businesses, instead of laboriously sifting through heaps of documents to train your chatbot, you can simply store them in a VectorDB. When a query arises, your chatbot can retrieve the pertinent information and present it in a user-friendly format.\n",
    "\n",
    "## RAG vs. Fine-Tuning\n",
    "\n",
    "- Choose Fine-Tuning when you need to refine your chatbot's behavior.\n",
    "- Opt for RAG when you want your chatbot to tap into external knowledge bases. Bonus: RAG works seamlessly even with smaller 7B models!\n",
    "\n",
    "Our journey begins with a beginner's example, addresses challenges, and delves into advanced use cases in the world of RAG. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install requests\n",
    "%pip install duckduckgo_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "from openai import OpenAI\n",
    "import IPython\n",
    "\n",
    "\n",
    "# Set the client and api key\n",
    "\n",
    "llm_client = OpenAI(\n",
    "    api_key=''\n",
    ")\n",
    "\n",
    "\n",
    "# Let's set our model\n",
    "MODEL = \"gpt-3.5-turbo\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, when utilizing zero-shot learning, the responses generated are rooted in the LLM's training data, which is limited to information available up to its training cutoff date in September 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-Shot\n",
    "\n",
    "question = 'Who is the CEO of Twitter in 2023?'\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "IPython.display.Markdown(response.choices[0].message.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's introduce a function that enables us to leverage DuckDuckGo and adapt the prompt in a way that allows us to present both the query and the search results concurrently..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG - DuckDuckGo\n",
    "from duckduckgo_search import DDGS\n",
    "from gptrim import trim\n",
    "\n",
    "def search_internet(query):\n",
    "    \"\"\"\n",
    "    Use Duckduck go Search\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 5\n",
    "    result_text=''\n",
    "    with DDGS() as ddgs:\n",
    "        search_results = [r for r in ddgs.text(query, max_results=count)]\n",
    "        for result in search_results:\n",
    "            title = result.get('title', '')\n",
    "            snippet = result.get('body', '')\n",
    "            url = result.get('href', '')\n",
    "            result_text += f'Title: {title}\\nSnippet: {snippet}\\nURL: {url}\\n\\n'\n",
    "\n",
    "        search_prompt = f\"\"\"\n",
    "        Based on the internet search results provided in <>, provide an answer to the query [] if it is relevant along with a source URL. \\\n",
    "        If there are no internet search results or if they are not relevant then say \\\"Please try again.\\\"\\\n",
    "        \n",
    "        context:<{trim(result_text)}>\n",
    "        query:[{query}]\n",
    "        \"\"\"\n",
    "\n",
    "        print ('Original text source:\\n'+result_text)\n",
    "\n",
    "        print ('Trimmed text source:\\n'+trim(result_text))\n",
    "\n",
    "        return {\"role\":\"system\",\"content\": search_prompt}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our function at our disposal, let's approach the task by first sending our query to the DuckDuckGo function and then supplying the adapted prompt to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEP 1 - ASK THE QUESTION\n",
    "question = 'Who is the CEO of Twitter in 2023?'\n",
    "\n",
    "\n",
    "# STEP 2 - SUBMIT THE QUESTION TO AN API TO GET RESULTS AND CREATE A NEW PROMPT FOR THE LLM\n",
    "\n",
    "prompt = search_internet(question)\n",
    "\n",
    "\n",
    "# STEP 4 - SEND THE MODIFIED PROMPT TO THE LLM \n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        prompt\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's delve into the challenges at hand:\n",
    "\n",
    "### Challenge 1 - Simplifying Complex Queries\n",
    "One prominent issue is the substantial text required to extract a concise answer. This not only impacts comprehension but also results in the usage of more tokens, potentially incurring additional costs. However, you can employ modules like \"gptrim\" to condense the text by approximately 50%, eliminating spaces, stop words, and the like. Remarkably, the LLM, particularly GPT-3 and 4, remains adept at understanding and responding effectively to such condensed input.\n",
    "\n",
    "### Challenge 2 - Transforming User Queries into Search Queries\n",
    "Accepting the user's query as-is doesn't always lead to optimal results. To address this, we need a mechanism to modify the query into an effective search query. Here, the LLM can assist in generating appropriate search queries using the function calling API. While several frameworks like Langchain and Semantic Kernel are available, I have found that function calling offers a straightforward and practical approach.\n",
    "\n",
    "### Challenge 3 - Efficient Decision-Making\n",
    "Consider that this setup operates within a chatbot application, across platforms such as websites, Microsoft Teams, Slack, Alexa, or robotic interfaces. In such scenarios, we don't want to trigger internet searches unnecessarily. To optimize this, we can introduce functions with distinct names and descriptions to the LLM's function calling API. The LLM then possesses the discretion to determine whether a function call is warranted, allowing for context-appropriate responses.\n",
    "\n",
    "### Challenge 4 - Accessing Company Documents\n",
    "Intriguingly, there's a desire to engage with company documents effectively. While you can explore this avenue with Confluence, Stack Overflow, and Sharepoint APIs, it's important to note that these solutions rely on keyword-based searches. This necessitates the exact matching of keywords found in document titles for retrieval. Alternatively, leveraging a vector database introduces the capacity for semantic searches. Here, you can import all your documents or text snippets and harness the power of semantic search by querying the concept or idea most closely aligned with the content you seek. We'll delve into a practical example of this in a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Function Calling\n",
    "\n",
    "In this approach, we entrust the LLM with the authority to discern when a function (often referred to as \"skills\") is necessary, generate the appropriate search query, and then incorporate the results into the LLM to obtain the final answer.\n",
    "\n",
    "Here's the breakdown of this process:\n",
    "\n",
    "1. **Function Development:** We create functions that act as additional \"eyes\" (read) and \"hands\" (actions/write) for the LLM.\n",
    "\n",
    "2. **Function Definitions:** We define these functions in a list, specifying the exact function name and a description. The LLM utilizes this information to select the relevant function. We also define the properties required for each function.\n",
    "\n",
    "3. **Query Submission:** Your query is submitted to the LLM, along with the function definitions.\n",
    "\n",
    "4. **Function Requirement:** If the LLM determines that a function is required, you will receive a response that includes the necessary function and its associated properties.\n",
    "\n",
    "5. **Function Execution:** You utilize the information provided by the LLM to internally trigger and execute the required function. This could involve searching the internet, performing specific actions, or any other defined tasks.\n",
    "\n",
    "6. **Final Answer:** After executing the function, you return to the LLM with the required information, and the LLM generates the final response.\n",
    "\n",
    "This approach streamlines the interaction between the LLM and the external functions, enhancing the model's ability to deliver precise and context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to run the search_internet function before this!\n",
    "import json\n",
    "\n",
    "## Requires a function \n",
    "QUESTION = \"Who is Twitter's CEO in 2023?\"\n",
    "## Doesn't require a function\n",
    "#QUESTION = \"How can i make a cup of tea?\"\n",
    "\n",
    "\n",
    "# Let us define the search_internet function for the LLM...\n",
    "skill_definitions = [\n",
    "      {\n",
    "        \"name\": \"search_internet\",\n",
    "        \"description\": \"Searches the internet to provide context to a query if required\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The query to search for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You have tools that add more up to date context to queries should they be required, otherwise act as a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": QUESTION}\n",
    "    ]\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=skill_definitions, # we present our skill definitions to the LLM. Think of it as a toolbox of functions :)\n",
    "    function_call='auto' # auto means that we let the LLM decide when to use a function. You can also set a static function name here so that it will always call it.\n",
    ")\n",
    "\n",
    "print (f'Look out for the function_call and finish_reason keys shown in the JSON below:\\n {response}') \n",
    "\n",
    "\n",
    "### Let us take the JSON response and load it\n",
    "\n",
    "\n",
    "### Do we need a skill?\n",
    "if 'function_call' in response.choices[0].finish_reason:\n",
    "\n",
    "    # tie response with our functions\n",
    "    available_functions = {\n",
    "        'search_internet': search_internet\n",
    "    }\n",
    "\n",
    "\n",
    "    # extract function name and arguments from the LLM\n",
    "    function_name = response.choices[0].message.function_call.name\n",
    "    function_args = json.loads(response.choices[0].message.function_call.arguments)\n",
    "\n",
    "    function2call = available_functions.get(function_name)\n",
    "\n",
    "    # call it appropriately\n",
    "    function_response = function2call(**function_args)\n",
    "\n",
    "    # append the response to the message history\n",
    "    messages.append(function_response)\n",
    "\n",
    "    # send the message history to the LLM again without functions.\n",
    "    new_response = llm_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    #functions=skill_definitions,\n",
    "    #function_call='auto' \n",
    ")\n",
    "    \n",
    "    # Result\n",
    "\n",
    "    print ('\\n\\n'+str(new_response.choices[0].message.content))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(response.choices[0].message.content) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident, the LLM has determined the necessity of a tool and has furnished the essential arguments for its execution. Notably, the query was also appropriately modified to align with this requirement.\n",
    "\n",
    "While this approach is effective, it's crucial to be vigilant, as the LLM may occasionally propose function names that do not exist. Implementing a conditional check to ensure that the returned function name aligns with the predefined ones is advisable to maintain control and accuracy.\n",
    "\n",
    "With this approach, you have the flexibility to integrate as many \"hands\" and \"eyes\" as necessary into your chatbot, enabling it to adapt and extend its capabilities as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
